{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwwelch/brain-waves-classification/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIIwPhfYdqoO",
        "outputId": "8e5765d8-3cb8-47cf-8108-ef98a232641b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "#import pyspark.sql.functions as F\n",
        "#from pyspark.sql import SparkSession\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBFHFHKp8HZY",
        "outputId": "5ac8a8d8-0feb-4d0d-b6f3-d0209e12bb7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Making the code device-agnostic\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Creating a test tensor\n",
        "x = torch.randint(1, 100, (100, 100)).to(device)\n",
        "\n",
        "#x = x.to(torch.device('cuda'))\n",
        "\n",
        "print(x.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TZP-uyXmha80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641e22ef-a12d-4b27-8eb5-e0cbd7237c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Seizure' 'Other' 'Other' 'LRDA']\n",
            "11138\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "\n",
        "vote_cols = [\"seizure_vote\", \"lpd_vote\", \"gpd_vote\", \"lrda_vote\", \"grda_vote\", \"other_vote\"]\n",
        "\n",
        "# Normalize the votes to get probabilities\n",
        "df[\"total_votes\"] = df[vote_cols].sum(axis=1)\n",
        "for col in vote_cols:\n",
        "    df[col + \"_prob\"] = df[col] / df[\"total_votes\"]\n",
        "\n",
        "# Keep only needed columns\n",
        "final_df = df[[\"spectrogram_id\", \"expert_consensus\"] + [col + \"_prob\" for col in vote_cols]]\n",
        "#print(final_df)\n",
        "# Drop duplicate eeg_ids â€” keep the first\n",
        "final_df = final_df.drop_duplicates(subset=\"spectrogram_id\", keep=\"first\")\n",
        "consensus = final_df[\"expert_consensus\"].values\n",
        "print(consensus[4:8])\n",
        "print(len(consensus))\n",
        "print(type(consensus))\n",
        "# Save to new CSV\n",
        "#final_df.to_csv(\"/Users/Patron/Documents/brain-waves-classification/hms_data/random_forest_saved_data/eeg_vote_probs_grouped.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwzA_kMwstHT",
        "outputId": "7af0b7f0-50b8-47a1-95dc-30dc7b71f3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]]\n",
            "11138\n",
            "<class 'numpy.ndarray'>\n",
            "[[0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# One Hot encoding\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "enc.fit(consensus.reshape(-1,1))\n",
        "consensus = enc.transform(consensus.reshape(-1,1)).toarray()\n",
        "print(consensus)\n",
        "print(len(consensus))\n",
        "print(type(consensus))\n",
        "print(consensus[4:8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P126B1iVd2Cs",
        "outputId": "42d8e738-663e-4e06-c4d4-4ea59855ad52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0 files\n",
            "Processed 100 files\n",
            "Processed 200 files\n",
            "Processed 300 files\n",
            "Processed 400 files\n",
            "Processed 500 files\n",
            "Processed 600 files\n",
            "Processed 700 files\n",
            "Processed 800 files\n",
            "Processed 900 files\n",
            "Processed 1000 files\n",
            "Processed 1100 files\n",
            "Processed 1200 files\n",
            "Processed 1300 files\n",
            "Processed 1400 files\n",
            "Processed 1500 files\n",
            "Processed 1600 files\n",
            "Processed 1700 files\n",
            "Processed 1800 files\n",
            "Processed 1900 files\n",
            "Processed 2000 files\n",
            "Processed 2100 files\n",
            "Processed 2200 files\n",
            "Processed 2300 files\n",
            "Processed 2400 files\n",
            "Processed 2500 files\n",
            "Processed 2600 files\n",
            "Processed 2700 files\n",
            "Processed 2800 files\n",
            "Processed 2900 files\n",
            "Processed 3000 files\n",
            "Processed 3100 files\n",
            "Processed 3200 files\n",
            "Processed 3300 files\n",
            "Processed 3400 files\n",
            "Processed 3500 files\n",
            "Processed 3600 files\n",
            "Processed 3700 files\n",
            "Processed 3800 files\n",
            "Processed 3900 files\n",
            "Processed 4000 files\n",
            "Processed 4100 files\n",
            "Processed 4200 files\n",
            "Processed 4300 files\n",
            "Processed 4400 files\n",
            "Processed 4500 files\n",
            "Processed 4600 files\n",
            "Processed 4700 files\n",
            "Processed 4800 files\n",
            "Processed 4900 files\n",
            "Processed 5000 files\n",
            "Processed 5100 files\n",
            "Processed 5200 files\n",
            "Processed 5300 files\n",
            "Processed 5400 files\n",
            "Processed 5500 files\n",
            "Processed 5600 files\n",
            "Processed 5700 files\n",
            "Processed 5800 files\n",
            "Processed 5900 files\n",
            "Processed 6000 files\n",
            "Processed 6100 files\n",
            "Processed 6200 files\n",
            "Processed 6300 files\n",
            "Processed 6400 files\n",
            "Processed 6500 files\n",
            "Processed 6600 files\n",
            "Processed 6700 files\n",
            "Processed 6800 files\n",
            "Processed 6900 files\n",
            "Processed 7000 files\n",
            "Processed 7100 files\n",
            "Processed 7200 files\n",
            "Processed 7300 files\n",
            "Processed 7400 files\n",
            "Processed 7500 files\n",
            "Processed 7600 files\n",
            "Processed 7700 files\n",
            "Processed 7800 files\n",
            "Processed 7900 files\n",
            "Processed 8000 files\n",
            "Processed 8100 files\n",
            "Processed 8200 files\n",
            "Processed 8300 files\n",
            "Processed 8400 files\n",
            "Processed 8500 files\n",
            "Processed 8600 files\n",
            "Processed 8700 files\n",
            "Processed 8800 files\n",
            "Processed 8900 files\n",
            "Processed 9000 files\n",
            "Processed 9100 files\n",
            "Processed 9200 files\n",
            "Processed 9300 files\n",
            "Processed 9400 files\n",
            "Processed 9500 files\n",
            "Processed 9600 files\n",
            "Processed 9700 files\n",
            "Processed 9800 files\n",
            "Processed 9900 files\n",
            "Processed 10000 files\n",
            "Processed 10100 files\n",
            "Processed 10200 files\n",
            "Processed 10300 files\n",
            "Processed 10400 files\n",
            "Processed 10500 files\n",
            "Processed 10600 files\n",
            "Processed 10700 files\n",
            "Processed 10800 files\n",
            "Processed 10900 files\n",
            "Processed 11000 files\n",
            "Processed 11100 files\n",
            "torch.Size([11138, 300, 401])\n"
          ]
        }
      ],
      "source": [
        "# Initiate array of parquet files\n",
        "df_parquet = torch.zeros([len(consensus),300,401])\n",
        "\n",
        "# Loop through parquet files and put into a tensor\n",
        "data_dir = Path('/content/drive/MyDrive/train_spectrograms')\n",
        "for i, parquet_path in enumerate(data_dir.glob('*.parquet')):\n",
        "  if i >= len(consensus):\n",
        "    break\n",
        "  else:\n",
        "    data = torch.from_numpy(pd.read_parquet(parquet_path).values)[:300,:]\n",
        "    data = torch.nan_to_num(data, nan=0.0)\n",
        "    df_parquet[i,:,:] = data\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Processed {i} files\")\n",
        "print(df_parquet.shape)\n",
        "assert not torch.isnan(df_parquet).any()\n",
        "# Access the parquet file\n",
        "#df_parquet = pd.read_parquet('/content/drive/MyDrive/train_spectrograms_sample')\n",
        "#print(df_parquet.shape)\n",
        "#print(df_parquet.type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCu4QUothhIB"
      },
      "outputs": [],
      "source": [
        "# Add noise\n",
        "def add_spectrogram_noise(input, amp = 1, spread = 0.1):\n",
        "  # Clipping the labels off\n",
        "  #df_data = input.iloc[:,1:]\n",
        "\n",
        "  # Calculate the mean\n",
        "  mu = torch.mean(input)\n",
        "\n",
        "  # Calculate the standard deviation and replace NaN with 0\n",
        "  std = torch.std(input)\n",
        "  std = torch.where(torch.isnan(std), torch.tensor(0.0, device=std.device), std) # Replace NaN with 0\n",
        "\n",
        "  # Add Gaussian noise\n",
        "  df_data = input + amp*torch.normal(mu, spread*std, size = input.shape)\n",
        "  return df_data\n",
        "\n",
        "# Scale the data\n",
        "def scale_spectrogram(input):\n",
        "  df_data = 10*input/input.max()\n",
        "  return df_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZWtvqrTQIak",
        "outputId": "287b86b0-de55-4c87-a9cd-bb62659eca60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6682, 6])\n"
          ]
        }
      ],
      "source": [
        "# Split data into training and test data\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df_parquet, consensus, test_size=0.2, random_state=42)\n",
        "train_amount = int(0.6*len(consensus))\n",
        "X_train = df_parquet[:train_amount,:,1:]\n",
        "X_test = df_parquet[train_amount:,:,1:]\n",
        "y_train = torch.from_numpy(consensus[:train_amount])\n",
        "y_test = torch.from_numpy(consensus[train_amount:])\n",
        "\n",
        "assert not torch.isnan(X_train).any()\n",
        "print(y_train.shape)\n",
        "#assert not torch.isnan(y_train).any()\n",
        "#print(torch.sum(X_train == 'nan').item())\n",
        "#print(X_train)\n",
        "# Add noise and scale training data\n",
        "#X_train = add_spectrogram_noise(X_train).to(device)\n",
        "#print(X_train)\n",
        "#X_train = scale_spectrogram(X_train).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ollVH_hYlDxz"
      },
      "outputs": [],
      "source": [
        "# Define Dataset\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, X_train, y_train):\n",
        "        self.data_labels = y_train.argmax(-1)\n",
        "        self.imgs = X_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.data_labels[idx]\n",
        "        image = self.imgs[idx, :, :]\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HdYC-kfXPf-2"
      },
      "outputs": [],
      "source": [
        "# X_train = scale_spectrogram(X_train).to(device)\n",
        "#X_test = scale_spectrogram(X_test).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vdP1c4nRiRyv"
      },
      "outputs": [],
      "source": [
        "# Dataloaders and Datasets\n",
        "training_data = EEGDataset(X_train, y_train)\n",
        "trainloader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "\n",
        "test_data = EEGDataset(X_test, y_test)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "l8V4sHbWQqxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e636cc-3480-4945-a7b5-d57aa572c558"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0021, -0.0011, -0.0003, -0.0045,  0.0006,  0.0064]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "# Define CNN (class)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 7, stride = 4, padding=3)\n",
        "        self.bn = nn.BatchNorm2d(6)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 7, stride = 4, padding=3)\n",
        "        #self.conv3 = nn.Conv2d(16, 64, 5, stride = 2)\n",
        "        self.fc1 = nn.Linear(480, 256)\n",
        "        # self.fc2 = nn.Softmax()\n",
        "        #self.fc1 = nn.Linear(6528, 6)\n",
        "        self.fc2 = nn.Linear(256, 84)\n",
        "        self.fc3 = nn.Linear(84, 6)\n",
        "\n",
        "        # Initialize weights\n",
        "        init.normal_(self.conv1.weight, mean=0., std=0.01)\n",
        "        init.normal_(self.conv2.weight, mean=0., std=0.01)\n",
        "        init.normal_(self.fc1.weight, mean=0., std=0.01)\n",
        "        init.normal_(self.fc2.weight, mean=0., std=0.01)\n",
        "        init.normal_(self.fc3.weight, mean=0., std=0.01)\n",
        "        init.zeros_(self.conv1.bias)\n",
        "        init.zeros_(self.conv2.bias)\n",
        "        init.zeros_(self.fc1.bias)\n",
        "        init.zeros_(self.fc2.bias)\n",
        "        init.zeros_(self.fc3.bias)\n",
        "        init.zeros_(self.bn.bias)\n",
        "        init.zeros_(self.bn2.bias)\n",
        "        init.ones_(self.bn.weight)\n",
        "        init.ones_(self.bn2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.pool(F.relu(self.bn(self.conv1(x))))\n",
        "        # assert torch.isnan(x.view(-1)).sum().item()==0\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        # assert torch.isnan(x.view(-1)).sum().item()==0\n",
        "        #x = self.pool(F.relu(self.conv3(x)))\n",
        "        # assert torch.isnan(x.view(-1)).sum().item()==0\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # print(x.shape)\n",
        "        # assert torch.isnan(x.view(-1)).sum().item()==0\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # assert torch.isnan(x.view(-1)).sum().item()==0\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        #assert torch.isnan(x.view(-1)).sum().item()==0\n",
        "        #x = self.fc3(x)\n",
        "        #assert torch.isnan(x.view(-1)).sum().item()==0\n",
        "        #print(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)\n",
        "\n",
        "x = torch.randn(1, 300, 400)\n",
        "net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4oQYM73op4e",
        "outputId": "6b29f12e-5f05-4c27-9b9a-49702d9cb1f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x78b58465fca0>\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "print(net.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plt.imshow(torch.log(inputs[2]).numpy())\n",
        "# torch.log10(torch.maximum(inputs, torch.tensor(1e-10))).max()\n",
        "def process_input(x):\n",
        "  min_val = torch.tensor(1e-10)\n",
        "  return torch.log10(torch.maximum(x, min_val))"
      ],
      "metadata": {
        "id": "PCDMKAApLZBg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YhFXfXO8o1Xg",
        "outputId": "fef18d7d-f832-425e-fb5d-8e2275f9167e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] Loss=1.79 pnorm=6.08 gnorm=0.38 onorm=0.03\n",
            "[20] Loss=1.78 pnorm=6.03 gnorm=0.38 onorm=0.29\n",
            "[40] Loss=1.75 pnorm=5.95 gnorm=0.43 onorm=0.75\n",
            "[60] Loss=1.74 pnorm=5.87 gnorm=0.32 onorm=1.22\n",
            "[80] Loss=1.73 pnorm=5.79 gnorm=0.28 onorm=1.68\n",
            "[100] Loss=1.72 pnorm=5.71 gnorm=0.29 onorm=2.09\n",
            "Epoch: 0 Loss: 1.754972776912507\n",
            "[105] Loss=1.74 pnorm=5.69 gnorm=0.17 onorm=2.18\n",
            "[125] Loss=1.69 pnorm=5.61 gnorm=0.32 onorm=2.57\n",
            "[145] Loss=1.65 pnorm=5.54 gnorm=0.38 onorm=2.98\n",
            "[165] Loss=1.69 pnorm=5.46 gnorm=0.23 onorm=3.40\n",
            "[185] Loss=1.65 pnorm=5.39 gnorm=0.29 onorm=3.74\n",
            "[205] Loss=1.63 pnorm=5.32 gnorm=0.33 onorm=4.09\n",
            "Epoch: 1 Loss: 1.686455339477176\n",
            "[210] Loss=1.63 pnorm=5.30 gnorm=0.33 onorm=4.18\n",
            "[230] Loss=1.65 pnorm=5.23 gnorm=0.24 onorm=4.55\n",
            "[250] Loss=1.73 pnorm=5.17 gnorm=0.13 onorm=4.96\n",
            "[270] Loss=1.56 pnorm=5.10 gnorm=0.44 onorm=5.47\n",
            "[290] Loss=1.64 pnorm=5.04 gnorm=0.23 onorm=5.95\n",
            "[310] Loss=1.63 pnorm=4.97 gnorm=0.26 onorm=6.46\n",
            "Epoch: 2 Loss: 1.636636598904928\n",
            "[315] Loss=1.54 pnorm=4.96 gnorm=0.69 onorm=6.61\n",
            "[335] Loss=1.56 pnorm=4.89 gnorm=0.43 onorm=7.16\n",
            "[355] Loss=1.52 pnorm=4.83 gnorm=0.61 onorm=8.01\n",
            "[375] Loss=1.52 pnorm=4.77 gnorm=0.75 onorm=8.49\n",
            "[395] Loss=1.57 pnorm=4.71 gnorm=0.47 onorm=8.83\n",
            "[415] Loss=1.46 pnorm=4.65 gnorm=0.57 onorm=9.35\n",
            "Epoch: 3 Loss: 1.590974523907616\n",
            "[420] Loss=1.57 pnorm=4.64 gnorm=0.62 onorm=9.61\n",
            "[440] Loss=1.69 pnorm=4.58 gnorm=1.09 onorm=10.03\n",
            "[460] Loss=1.58 pnorm=4.51 gnorm=0.26 onorm=10.17\n",
            "[480] Loss=1.50 pnorm=4.46 gnorm=0.87 onorm=10.13\n",
            "[500] Loss=1.60 pnorm=4.40 gnorm=0.80 onorm=9.96\n",
            "[520] Loss=1.55 pnorm=4.34 gnorm=0.36 onorm=10.47\n",
            "Epoch: 4 Loss: 1.5750333377293175\n",
            "[525] Loss=1.71 pnorm=4.33 gnorm=0.69 onorm=10.14\n",
            "[545] Loss=1.57 pnorm=4.27 gnorm=0.65 onorm=10.66\n",
            "[565] Loss=1.56 pnorm=4.21 gnorm=0.55 onorm=10.41\n",
            "[585] Loss=1.66 pnorm=4.16 gnorm=0.56 onorm=10.80\n",
            "[605] Loss=1.51 pnorm=4.10 gnorm=0.44 onorm=10.34\n",
            "[625] Loss=1.66 pnorm=4.05 gnorm=0.78 onorm=10.69\n",
            "Epoch: 5 Loss: 1.5731905642009913\n",
            "[630] Loss=1.62 pnorm=4.04 gnorm=0.36 onorm=10.70\n",
            "[650] Loss=1.49 pnorm=3.98 gnorm=0.57 onorm=10.69\n",
            "[670] Loss=1.41 pnorm=3.93 gnorm=0.91 onorm=11.05\n",
            "[690] Loss=1.53 pnorm=3.88 gnorm=0.76 onorm=10.10\n",
            "[710] Loss=1.56 pnorm=3.83 gnorm=0.33 onorm=11.36\n",
            "[730] Loss=1.50 pnorm=3.78 gnorm=0.51 onorm=11.07\n",
            "Epoch: 6 Loss: 1.5706005141848611\n",
            "[735] Loss=1.65 pnorm=3.77 gnorm=1.35 onorm=11.02\n",
            "[755] Loss=1.57 pnorm=3.72 gnorm=0.39 onorm=10.51\n",
            "[775] Loss=1.39 pnorm=3.67 gnorm=1.17 onorm=11.19\n",
            "[795] Loss=1.62 pnorm=3.62 gnorm=0.65 onorm=11.43\n",
            "[815] Loss=1.59 pnorm=3.58 gnorm=0.52 onorm=11.24\n",
            "[835] Loss=1.54 pnorm=3.53 gnorm=0.47 onorm=10.85\n",
            "Epoch: 7 Loss: 1.5692504156203497\n",
            "[840] Loss=1.55 pnorm=3.52 gnorm=0.71 onorm=10.73\n",
            "[860] Loss=1.74 pnorm=3.47 gnorm=0.89 onorm=10.81\n",
            "[880] Loss=1.57 pnorm=3.43 gnorm=0.53 onorm=11.00\n",
            "[900] Loss=1.78 pnorm=3.39 gnorm=0.85 onorm=11.19\n",
            "[920] Loss=1.55 pnorm=3.34 gnorm=0.47 onorm=10.89\n",
            "[940] Loss=1.64 pnorm=3.30 gnorm=0.49 onorm=11.18\n",
            "Epoch: 8 Loss: 1.570963412239438\n",
            "[945] Loss=1.59 pnorm=3.29 gnorm=0.71 onorm=11.11\n",
            "[965] Loss=1.46 pnorm=3.25 gnorm=0.59 onorm=11.21\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-e8c7f7938730>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0;31m#print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mp_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mg_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "\n",
        "net = Net().to(device)\n",
        "# optimizer = optim.Adam(net.parameters(), lr=.0001)\n",
        "optimizer = optim.SGD(net.parameters(), lr=.001, momentum=0.9, weight_decay=0.07)\n",
        "\n",
        "for epoch in range(100):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.type(torch.FloatTensor).to(device)\n",
        "        inputs = process_input(inputs)\n",
        "        labels = labels.long().to(device)\n",
        "        assert torch.isnan(inputs.view(-1)).sum().item()==0\n",
        "        assert torch.isnan(labels.view(-1)).sum().item()==0\n",
        "        #print(data)\n",
        "        #print(i)\n",
        "        #print(inputs)\n",
        "        #print(labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #print(i)\n",
        "        outputs = net(inputs)\n",
        "        #if i == 0:\n",
        "        #print(outputs)\n",
        "        assert torch.isnan(outputs.view(-1)).sum().item()==0\n",
        "        #if i == 0 and epoch == 0:\n",
        "          #print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        p_norm = torch.sum(torch.stack([torch.norm(p)**2 for p in net.parameters()])).sqrt()\n",
        "        g_norm = torch.sum(torch.stack([torch.norm(p.grad)**2 for p in net.parameters()])).sqrt()\n",
        "        o_norm = torch.norm(outputs)\n",
        "\n",
        "        if i % 20 == 0:\n",
        "          print(f\"[{i + epoch * len(trainloader)}] Loss={loss.item():.2f} pnorm={p_norm.item():.2f} gnorm={g_norm.item():.2f} onorm={o_norm.item():.2f}\")\n",
        "        #if i == 0 and epoch == 0:\n",
        "          #print(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() / len(trainloader)\n",
        "        #print(running_loss)\n",
        "    print('Epoch: ' + str(epoch) + ' Loss: ' + str(running_loss))\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "        images, labels = data\n",
        "        images = images.type(torch.FloatTensor).to(device)\n",
        "        #print(images.shape)\n",
        "        images = process_input(images)\n",
        "        #print(labels.shape)\n",
        "        labels = labels.long().to(device)\n",
        "        outputs = net(images)\n",
        "        #print(outputs.shape)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "          #if i % 100 == 0:\n",
        "            #print(predicted.item())\n",
        "        #print(predicted)\n",
        "        #print(labels[i].item())\n",
        "          #print(range(i))\n",
        "        #print(predicted.item())\n",
        "        for j in range(len(predicted)):\n",
        "          if predicted[j].item() == labels[j].item():\n",
        "            correct += 1\n",
        "          total += 1\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a04xI_ikQjEN",
        "outputId": "9bec0a58-746b-4a1e-fdf8-dce844941721"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 41 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMqqOTbgjVLtz8H4IwhkZDP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}